package com.aadharanalysis;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class DatasetAadharAnalysis {

	public static void main(String[] args) {

		SparkSession spark=SparkSession.builder().appName("aadhar analysis").master("local").getOrCreate();
		
		Dataset<Row> input=spark.read().option("header","true").option("inferschema", "true")
				.csv("/home/purva/spark-data/VacationProjects/aadhaar-dataset-analysis/data/input.csv");
		//input.printSchema();
		
//Count the number of identities(Aadhaar) generated in each state
		Dataset<Row> ans1=input.groupBy(input.col("state")).agg(org.apache.spark.sql.functions.sum("Aadhaar generated"));
		//ans1.show();
		
//Count the number of identities(Aadhaar) generated by each Enrollment Agency
		Dataset<Row> ans2=input.groupBy(input.col("Enrolment Agency")).agg(org.apache.spark.sql.functions.sum("Aadhaar generated"));
		//ans2.show();
		
//Top 10 districts with maximum identities generated for both Male and Female
		Dataset<Row> ans3=input.groupBy(input.col("district"),input.col("gender"))
				.agg(org.apache.spark.sql.functions.sum("Aadhaar generated").as("sum"));
		//ans3.sort(ans3.col("sum").desc()).show();
		
//enrollment rejected gender wise in a distric
		Dataset<Row> ans4=input.groupBy(input.col("district"), input.col("gender"))
		.agg(org.apache.spark.sql.functions.sum("Enrolment Rejected").as("totalRejecyed"));
		//ans4.show();
		
//top 10 districts whose enrollment rejection  is high for age above 10 years 
		Dataset<Row> ans51=input.filter(input.col("age").gt(10));
		Dataset<Row> ans52=ans51.groupBy(ans51.col("district"))
				.agg(org.apache.spark.sql.functions.sum("Enrolment Rejected").as("sum"));
		Dataset<Row> ans5=ans52.sort(ans52.col("sum").desc());
		//ans5.show();	
		
//no of adhars generated and rejected :- mobile and email (both,mobile,email,none)
		Dataset<Row> ans611=input.filter(input.col("Residents providing email").equalTo(1));
		Dataset<Row> ans612=ans611.groupBy(ans611.col("district"))
				.agg(org.apache.spark.sql.functions.sum("Enrolment Rejected").as("EmailRejected"),
						org.apache.spark.sql.functions.sum(ans611.col("Aadhaar generated")).as("EmailGenerated"));
		//ans612.show();
		
		Dataset<Row> ans621=input.filter(input.col("Residents providing mobile number").equalTo(1));
		Dataset<Row> ans622=ans611.groupBy(ans621.col("district"))
				.agg(org.apache.spark.sql.functions.sum("Enrolment Rejected").as("MobileRejected"),
					org.apache.spark.sql.functions.sum(ans621.col("Aadhaar generated")).as("MobileGenerated"));
		//ans622.show();
		
		Dataset<Row> ans631=input.filter(input.col("Residents providing mobile number").equalTo(1)
				.and(input.col("Residents providing email").equalTo(1)));
		Dataset<Row> ans632=ans611.groupBy(ans631.col("district"))
				.agg(org.apache.spark.sql.functions.sum("Enrolment Rejected").as("EmailMobileRejected"),
					org.apache.spark.sql.functions.sum(ans621.col("Aadhaar generated")).as("EmailMobileGenerated"));
		//ans632.show();
		
		Dataset<Row> ans6=ans612.join(ans622,"district").join(ans632,"district");
		ans6.show();
	
		for(int i=0;i<10;i++) {
			Dataset<Row> ans6111=input.filter(input.col("Residents providing email").equalTo(i));
		}
		
	}

}
